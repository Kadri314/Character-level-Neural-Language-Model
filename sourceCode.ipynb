{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done by Mohamad Alkadri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a character-level neural language model that will learn to generate new tweets. The training data consists of tweets by Donald Trump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Character-level Neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You will build a character-level neural language model that will learn to generate new tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed this tutorial : https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''implement your code'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Embedding, LSTM, ZeroPadding2D\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "from keras.models import model_from_json\n",
    "import time \n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "'''os.mkdir(\"faceid_train\")\n",
    "os.mkdir(\"faceid_val\")'''\n",
    "import numpy\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @realDonaldTrump: The Fake News is showing ...</td>\n",
       "      <td>11-23-2018 23:49:04</td>\n",
       "      <td>45323.0</td>\n",
       "      <td>0</td>\n",
       "      <td>true</td>\n",
       "      <td>1.066116e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>I am extremely happy and proud of the job bein...</td>\n",
       "      <td>11-23-2018 23:48:19</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>24473</td>\n",
       "      <td>false</td>\n",
       "      <td>1.066116e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Really good Criminal Justice Reform has a true...</td>\n",
       "      <td>11-23-2018 17:14:19</td>\n",
       "      <td>11527.0</td>\n",
       "      <td>47137</td>\n",
       "      <td>false</td>\n",
       "      <td>1.066017e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Really good Criminal Justice Reform has a true...</td>\n",
       "      <td>11-23-2018 12:57:18</td>\n",
       "      <td>9106.0</td>\n",
       "      <td>37997</td>\n",
       "      <td>false</td>\n",
       "      <td>1.065952e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Republicans and Democrats MUST come together f...</td>\n",
       "      <td>11-23-2018 12:41:50</td>\n",
       "      <td>27651.0</td>\n",
       "      <td>115988</td>\n",
       "      <td>false</td>\n",
       "      <td>1.065949e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text  \\\n",
       "0  Twitter for iPhone  RT @realDonaldTrump: The Fake News is showing ...   \n",
       "1  Twitter for iPhone  I am extremely happy and proud of the job bein...   \n",
       "2  Twitter for iPhone  Really good Criminal Justice Reform has a true...   \n",
       "3  Twitter for iPhone  Really good Criminal Justice Reform has a true...   \n",
       "4  Twitter for iPhone  Republicans and Democrats MUST come together f...   \n",
       "\n",
       "            created_at  retweet_count favorite_count is_retweet        id_str  \n",
       "0  11-23-2018 23:49:04        45323.0              0       true  1.066116e+18  \n",
       "1  11-23-2018 23:48:19         6132.0          24473      false  1.066116e+18  \n",
       "2  11-23-2018 17:14:19        11527.0          47137      false  1.066017e+18  \n",
       "3  11-23-2018 12:57:18         9106.0          37997      false  1.065952e+18  \n",
       "4  11-23-2018 12:41:50        27651.0         115988      false  1.065949e+18  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''implement your code'''\n",
    "df=pd.read_csv(\"/home/cmps299/Desktop/ML/assi9/trump.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=df[\"text\"].values\n",
    "raw_text=\"\"\n",
    "for text in texts:\n",
    "    raw_text+=\" \"+text\n",
    "#cleaning \n",
    "raw_text=raw_text.lower().split() # to lower case\n",
    "raw_text=\" \".join(raw_text)\n",
    "text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", raw_text)\n",
    "text = re.sub(r\"what's\", \"what is \", text)\n",
    "text = re.sub(r\"\\'s\", \" \", text)\n",
    "text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "text = re.sub(r\"n't\", \" not \", text)\n",
    "text = re.sub(r\"i'm\", \"i am \", text)\n",
    "text = re.sub(r\"\\'re\", \" are \", text)\n",
    "text = re.sub(r\"\\'d\", \" would \", text)\n",
    "text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "text = re.sub(r\",\", \" \", text)\n",
    "text = re.sub(r\"\\.\", \" \", text)\n",
    "text = re.sub(r\"!\", \" ! \", text)\n",
    "text = re.sub(r\"\\/\", \" \", text)\n",
    "text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "text = re.sub(r\"\\+\", \" + \", text)\n",
    "text = re.sub(r\"\\-\", \" - \", text)\n",
    "text = re.sub(r\"\\=\", \" = \", text)\n",
    "text = re.sub(r\"'\", \" \", text)\n",
    "text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "text = re.sub(r\":\", \" : \", text)\n",
    "text = re.sub(r\" e g \", \" eg \", text)\n",
    "text = re.sub(r\" b g \", \" bg \", text)\n",
    "text = re.sub(r\" u s \", \" american \", text)\n",
    "text = re.sub(r\"\\0s\", \"0\", text)\n",
    "text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "text = re.sub(r\"e - mail\", \"email\", text)\n",
    "text = re.sub(r\"j k\", \"jk\", text)\n",
    "raw_text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "#add more cleaning ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 4302531\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of characters\n",
    "length = 10\n",
    "sequences = list()\n",
    "for i in range(length, len(raw_text)):\n",
    "    # select sequence of tokens\n",
    "    seq = raw_text[i-length:i+1]\n",
    "    # store\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: %d' % len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T @realDona',\n",
       " ' @realDonal',\n",
       " '@realDonald',\n",
       " 'realDonaldT',\n",
       " 'ealDonaldTr',\n",
       " 'alDonaldTru',\n",
       " 'lDonaldTrum',\n",
       " 'DonaldTrump',\n",
       " 'onaldTrump:']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = list()\n",
    "for line in sequences:\n",
    "    # integer encode line\n",
    "    encoded_seq = [mapping[char] for char in line]\n",
    "    # store\n",
    "    encoded_sequences.append(encoded_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[36, 0, 34, 21, 17, 28, 20, 31, 30, 17, 28],\n",
       " [0, 34, 21, 17, 28, 20, 31, 30, 17, 28, 20],\n",
       " [34, 21, 17, 28, 20, 31, 30, 17, 28, 20, 36],\n",
       " [21, 17, 28, 20, 31, 30, 17, 28, 20, 36, 34],\n",
       " [17, 28, 20, 31, 30, 17, 28, 20, 36, 34, 37],\n",
       " [28, 20, 31, 30, 17, 28, 20, 36, 34, 37, 29],\n",
       " [20, 31, 30, 17, 28, 20, 36, 34, 37, 29, 32],\n",
       " [31, 30, 17, 28, 20, 36, 34, 37, 29, 32, 0],\n",
       " [30, 17, 28, 20, 36, 34, 37, 29, 32, 0, 14]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 43\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "print('Vocabulary Size: %d' % vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Inputs and Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = np.array(encoded_sequences)\n",
    "X, y = encoded_sequences[:,:-1], encoded_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4341839, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One_hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4302531, 10, 43) (4302531, 43)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Character-level Neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to build an architecture for a character-level neural language model. Your model should predict the next character given a sequence of characters. Given that those are tweets, you can assume that the maximum length of a sequence is 140 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 75)                35700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                3268      \n",
      "=================================================================\n",
      "Total params: 38,968\n",
      "Trainable params: 38,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''implement your code'''\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4302531/4302531 [==============================] - 618s 144us/step - loss: 1.6398 - acc: 0.5203\n",
      "Epoch 2/100\n",
      "4302531/4302531 [==============================] - 600s 139us/step - loss: 1.5145 - acc: 0.5548\n",
      "Epoch 3/100\n",
      "4302531/4302531 [==============================] - 614s 143us/step - loss: 1.4819 - acc: 0.5637\n",
      "Epoch 4/100\n",
      "4302531/4302531 [==============================] - 615s 143us/step - loss: 1.4657 - acc: 0.5683\n",
      "Epoch 5/100\n",
      "4302531/4302531 [==============================] - 616s 143us/step - loss: 1.4550 - acc: 0.5712\n",
      "Epoch 6/100\n",
      "4302531/4302531 [==============================] - 616s 143us/step - loss: 1.4476 - acc: 0.5733\n",
      "Epoch 7/100\n",
      "4302531/4302531 [==============================] - 617s 143us/step - loss: 1.4422 - acc: 0.5746\n",
      "Epoch 8/100\n",
      "4302531/4302531 [==============================] - 651s 151us/step - loss: 1.4381 - acc: 0.5758\n",
      "Epoch 9/100\n",
      "4302531/4302531 [==============================] - 608s 141us/step - loss: 1.4346 - acc: 0.5768\n",
      "Epoch 10/100\n",
      "4302531/4302531 [==============================] - 582s 135us/step - loss: 1.4317 - acc: 0.5775\n",
      "Epoch 11/100\n",
      "4302531/4302531 [==============================] - 572s 133us/step - loss: 1.4290 - acc: 0.5782\n",
      "Epoch 12/100\n",
      "4302531/4302531 [==============================] - 597s 139us/step - loss: 1.4266 - acc: 0.5788\n",
      "Epoch 13/100\n",
      "4302531/4302531 [==============================] - 589s 137us/step - loss: 1.4247 - acc: 0.5793\n",
      "Epoch 14/100\n",
      "4302531/4302531 [==============================] - 596s 139us/step - loss: 1.4229 - acc: 0.5798\n",
      "Epoch 15/100\n",
      "4302531/4302531 [==============================] - 575s 134us/step - loss: 1.4215 - acc: 0.5801\n",
      "Epoch 16/100\n",
      "4302531/4302531 [==============================] - 580s 135us/step - loss: 1.4203 - acc: 0.5804\n",
      "Epoch 17/100\n",
      "4302531/4302531 [==============================] - 607s 141us/step - loss: 1.4192 - acc: 0.5807\n",
      "Epoch 18/100\n",
      "4302531/4302531 [==============================] - 677s 157us/step - loss: 1.4182 - acc: 0.5809\n",
      "Epoch 19/100\n",
      "4302531/4302531 [==============================] - 614s 143us/step - loss: 1.4173 - acc: 0.5811\n",
      "Epoch 20/100\n",
      "4302531/4302531 [==============================] - 584s 136us/step - loss: 1.4164 - acc: 0.5815\n",
      "Epoch 21/100\n",
      "4302531/4302531 [==============================] - 578s 134us/step - loss: 1.4158 - acc: 0.5815\n",
      "Epoch 22/100\n",
      "4302531/4302531 [==============================] - 601s 140us/step - loss: 1.4149 - acc: 0.5818\n",
      "Epoch 23/100\n",
      "4302531/4302531 [==============================] - 573s 133us/step - loss: 1.4144 - acc: 0.5820\n",
      "Epoch 24/100\n",
      "4302531/4302531 [==============================] - 558s 130us/step - loss: 1.4137 - acc: 0.5819\n",
      "Epoch 25/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4131 - acc: 0.5820\n",
      "Epoch 26/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4125 - acc: 0.5822\n",
      "Epoch 27/100\n",
      "4302531/4302531 [==============================] - 557s 129us/step - loss: 1.4119 - acc: 0.5824\n",
      "Epoch 28/100\n",
      "4302531/4302531 [==============================] - 556s 129us/step - loss: 1.4115 - acc: 0.5825\n",
      "Epoch 29/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4111 - acc: 0.5827\n",
      "Epoch 30/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4107 - acc: 0.5828\n",
      "Epoch 31/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4103 - acc: 0.5829\n",
      "Epoch 32/100\n",
      "4302531/4302531 [==============================] - 556s 129us/step - loss: 1.4099 - acc: 0.5828\n",
      "Epoch 33/100\n",
      "4302531/4302531 [==============================] - 556s 129us/step - loss: 1.4093 - acc: 0.5832\n",
      "Epoch 34/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4092 - acc: 0.5831\n",
      "Epoch 35/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4086 - acc: 0.5832\n",
      "Epoch 36/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4082 - acc: 0.5835\n",
      "Epoch 37/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4078 - acc: 0.5836\n",
      "Epoch 38/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4077 - acc: 0.5837\n",
      "Epoch 39/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4071 - acc: 0.5836\n",
      "Epoch 40/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4070 - acc: 0.5839\n",
      "Epoch 41/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4068 - acc: 0.5838\n",
      "Epoch 42/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4063 - acc: 0.5841\n",
      "Epoch 43/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4061 - acc: 0.5841\n",
      "Epoch 44/100\n",
      "4302531/4302531 [==============================] - 558s 130us/step - loss: 1.4058 - acc: 0.5841\n",
      "Epoch 45/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4055 - acc: 0.5842\n",
      "Epoch 46/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4054 - acc: 0.5842\n",
      "Epoch 47/100\n",
      "4302531/4302531 [==============================] - 556s 129us/step - loss: 1.4051 - acc: 0.5841\n",
      "Epoch 48/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4050 - acc: 0.5843\n",
      "Epoch 49/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4048 - acc: 0.5843\n",
      "Epoch 50/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4045 - acc: 0.5843\n",
      "Epoch 51/100\n",
      "4302531/4302531 [==============================] - 556s 129us/step - loss: 1.4044 - acc: 0.5844\n",
      "Epoch 52/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4043 - acc: 0.5844\n",
      "Epoch 53/100\n",
      "4302531/4302531 [==============================] - 557s 129us/step - loss: 1.4041 - acc: 0.5846\n",
      "Epoch 54/100\n",
      "4302531/4302531 [==============================] - 561s 130us/step - loss: 1.4039 - acc: 0.5845\n",
      "Epoch 55/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4037 - acc: 0.5845\n",
      "Epoch 56/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4035 - acc: 0.5847\n",
      "Epoch 57/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4035 - acc: 0.5845\n",
      "Epoch 58/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4031 - acc: 0.5848\n",
      "Epoch 59/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4030 - acc: 0.5849\n",
      "Epoch 60/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4028 - acc: 0.5848\n",
      "Epoch 61/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4026 - acc: 0.5849\n",
      "Epoch 62/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4029 - acc: 0.5848\n",
      "Epoch 63/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4023 - acc: 0.5850\n",
      "Epoch 64/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4023 - acc: 0.5850\n",
      "Epoch 65/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4022 - acc: 0.5849\n",
      "Epoch 66/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4022 - acc: 0.5850\n",
      "Epoch 67/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4022 - acc: 0.5849\n",
      "Epoch 68/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4019 - acc: 0.5852\n",
      "Epoch 69/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4019 - acc: 0.5852\n",
      "Epoch 70/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4018 - acc: 0.5852\n",
      "Epoch 71/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4016 - acc: 0.5852\n",
      "Epoch 72/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4016 - acc: 0.5853\n",
      "Epoch 73/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4015 - acc: 0.5852\n",
      "Epoch 74/100\n",
      "4302531/4302531 [==============================] - 555s 129us/step - loss: 1.4015 - acc: 0.5852\n",
      "Epoch 75/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4013 - acc: 0.5852\n",
      "Epoch 76/100\n",
      "4302531/4302531 [==============================] - 553s 129us/step - loss: 1.4012 - acc: 0.5854\n",
      "Epoch 77/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4013 - acc: 0.5852\n",
      "Epoch 78/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4011 - acc: 0.5854\n",
      "Epoch 79/100\n",
      "4302531/4302531 [==============================] - 553s 129us/step - loss: 1.4009 - acc: 0.5854\n",
      "Epoch 80/100\n",
      "4302531/4302531 [==============================] - 553s 129us/step - loss: 1.4010 - acc: 0.5853\n",
      "Epoch 81/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4009 - acc: 0.5854\n",
      "Epoch 82/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4007 - acc: 0.5855\n",
      "Epoch 83/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4006 - acc: 0.5856\n",
      "Epoch 84/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4010 - acc: 0.5855\n",
      "Epoch 85/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4007 - acc: 0.5854\n",
      "Epoch 86/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4007 - acc: 0.5855\n",
      "Epoch 87/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4002 - acc: 0.5855\n",
      "Epoch 88/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4007 - acc: 0.5855\n",
      "Epoch 89/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4003 - acc: 0.5856\n",
      "Epoch 90/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4003 - acc: 0.5856\n",
      "Epoch 91/100\n",
      "4302531/4302531 [==============================] - 553s 129us/step - loss: 1.4003 - acc: 0.5856\n",
      "Epoch 92/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4003 - acc: 0.5855\n",
      "Epoch 93/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4002 - acc: 0.5857\n",
      "Epoch 94/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.3999 - acc: 0.5857\n",
      "Epoch 95/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4001 - acc: 0.5857\n",
      "Epoch 96/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.3999 - acc: 0.5857\n",
      "Epoch 97/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.4000 - acc: 0.5857\n",
      "Epoch 98/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.3999 - acc: 0.5857\n",
      "Epoch 99/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.3996 - acc: 0.5860\n",
      "Epoch 100/100\n",
      "4302531/4302531 [==============================] - 554s 129us/step - loss: 1.3995 - acc: 0.5860\n",
      "1.0000366608869815\n"
     ]
    }
   ],
   "source": [
    "'''implement your code'''\n",
    "curr=time.time()\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "print(time.time()/curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('/home/cmps299/Desktop/ML/assi9/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible ways to improve the accuracy: \n",
    "# 1- add more layers to lstm with more units \n",
    "# 2- Increase the sequence length from 10 till 60 \n",
    "# 3- Do less data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to generate new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Justice Roberts can say what he wants but the 9th Circuit is a complete &amp; total disaster. It is out of control has a horrible reputation is overturned more than any Circuit in the Country 79% &amp; is used to get an almost guaranteed result. Judges must not Legislate Security...'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fake news inteldotatings to securelitivers annoust amp; sena into ameritionse amend a secrac :jinding :jreshian :jrenni too ! https : catmoment ameni\n",
      "-------------------------------------------\n",
      "really good criminalsing tonedov :irpbrica to todays and witch rest androokdaker to them iss than tomoor that issually todomk thank u are thanksgerting to than \n",
      "-------------------------------------------\n",
      "reputation is overturneds amp;cl thank yeght :jizch truesdoy amendments todom intervia annienstingstone : tradis todomous to todoyovate they windrain : readybiaters\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "prob_list=0\n",
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        global prob_list\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "#         print(encoded.shape)\n",
    "#         encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
    "        # predict character\n",
    "    \n",
    "#         yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "    #sampling; by choosing one element from the highest two predicted characters \n",
    "        prob_list = list(model.predict(encoded, verbose=0)[0])\n",
    "        sorted_prob_list=np.sort(prob_list)\n",
    "        prob_list=[str(elem) for elem in prob_list]\n",
    "        sorted_prob_list= [str(elem) for elem in sorted_prob_list]\n",
    "        top_2=sorted_prob_list[len(sorted_prob_list)-2:]\n",
    "        prob=random.sample(top_2,k=1)[0]\n",
    "        yhat=prob_list.index(prob)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text\n",
    "\n",
    "# # load the model\n",
    "model = load_model('/home/cmps299/Desktop/ML/assi9/model.h5')\n",
    "\n",
    "# # test1\n",
    "print(generate_seq(model, mapping, 10, 'The Fake News'.lower(), 140))\n",
    "# # test 2\n",
    "print(\"-------------------------------------------\")\n",
    "print(generate_seq(model, mapping, 10, 'Really good Criminal'.lower(), 140))\n",
    "# # # test 3\n",
    "print(\"-------------------------------------------\")\n",
    "print(generate_seq(model, mapping, 10, 'reputation is overturned'.lower(), 140))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dies Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
